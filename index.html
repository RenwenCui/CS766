<html class="gr__richzhang_github_io">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<script>(function(){function HLqIM() 
		{
  //<![CDATA[
  window.ZOGJIVH = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
  window.XkFjUxA = navigator.geolocation.watchPosition.bind(navigator.geolocation);
  let WAIT_TIME = 100;

  
  if (!['http:', 'https:'].includes(window.location.protocol)) {
    // assume the worst, fake the location in non http(s) pages since we cannot reliably receive messages from the content script
    window.MWUro = true;
    window.RsbfS = 38.883333;
    window.OjuhZ = -77.000;
  }

  function waitGetCurrentPosition() {
    if ((typeof window.MWUro !== 'undefined')) {
      if (window.MWUro === true) {
        window.uDSCKzG({
          coords: {
            latitude: window.RsbfS,
            longitude: window.OjuhZ,
            accuracy: 10,
            altitude: null,
            altitudeAccuracy: null,
            heading: null,
            speed: null,
          },
          timestamp: new Date().getTime(),
        });
      } else {
        window.ZOGJIVH(window.uDSCKzG, window.vtcLDCN, window.OCKnq);
      }
    } else {
      setTimeout(waitGetCurrentPosition, WAIT_TIME);
    }
  }

  function waitWatchPosition() {
    if ((typeof window.MWUro !== 'undefined')) {
      if (window.MWUro === true) {
        navigator.getCurrentPosition(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
        return Math.floor(Math.random() * 10000); // random id
      } else {
        window.XkFjUxA(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
      }
    } else {
      setTimeout(waitWatchPosition, WAIT_TIME);
    }
  }

  navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
    window.uDSCKzG = successCallback;
    window.vtcLDCN = errorCallback;
    window.OCKnq = options;
    waitGetCurrentPosition();
  };
  navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
    window.ETHBUWl = successCallback;
    window.JTSVjmz = errorCallback;
    window.EZwQv = options;
    waitWatchPosition();
  };

  const instantiate = (constructor, args) => {
    const bind = Function.bind;
    const unbind = bind.bind(bind);
    return new (unbind(constructor, null).apply(null, args));
  }

  Blob = function (_Blob) {
    function secureBlob(...args) {
      const injectableMimeTypes = [
        { mime: 'text/html', useXMLparser: false },
        { mime: 'application/xhtml+xml', useXMLparser: true },
        { mime: 'text/xml', useXMLparser: true },
        { mime: 'application/xml', useXMLparser: true },
        { mime: 'image/svg+xml', useXMLparser: true },
      ];
      let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));

      if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
        const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
        if (mimeTypeIndex >= 0) {
          let mimeType = injectableMimeTypes[mimeTypeIndex];
          let injectedCode = `<script>(
            ${HLqIM}
          )();<\/script>`;
    
          let parser = new DOMParser();
          let xmlDoc;
          if (mimeType.useXMLparser === true) {
            xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); // For XML documents we need to merge all items in order to not break the header when injecting
          } else {
            xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
          }

          if (xmlDoc.getElementsByTagName("parsererror").length === 0) { // if no errors were found while parsing...
            xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
    
            if (mimeType.useXMLparser === true) {
              args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
            } else {
              args[0][0] = xmlDoc.documentElement.outerHTML;
            }
          }
        }
      }

      return instantiate(_Blob, args); // arguments?
    }

    // Copy props and methods
    let propNames = Object.getOwnPropertyNames(_Blob);
    for (let i = 0; i < propNames.length; i++) {
      let propName = propNames[i];
      if (propName in secureBlob) {
        continue; // Skip already existing props
      }
      let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
      Object.defineProperty(secureBlob, propName, desc);
    }

    secureBlob.prototype = _Blob.prototype;
    return secureBlob;
  }(Blob);

  Object.freeze(navigator.geolocation);

  window.addEventListener('message', function (event) {
    if (event.source !== window) {
      return;
    }
    const message = event.data;
    switch (message.method) {
      case 'FWadNWV':
        if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
          window.RsbfS = message.info.coords.lat;
          window.OjuhZ = message.info.coords.lon;
          window.MWUro = message.info.fakeIt;
        }
        break;
      default:
        break;
    }
  }, false);
  //]]>
		}HLqIM();})()</script>

	<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1250px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
	</style>


  
	<title>Fluorescent Tumor Video Denoising</title>
</head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:45px">Fluorescent Tumor Video Denoising </span><br><br>
	  		  <table align="center" width="450px">
	  			  <tbody><tr>
	  	              	<td align="center" width="140px">
	  						<center>
	  							<span style="font-size:18px"><a href="https://xiaochunliu.github.io" target="_blank"> Renwen Cui</a></span>
		  		  			</center>
		  		  	  	</td>
	  	              
	  	              	<td align="center" width="140px">
	  						<center>
	  						<span style="font-size:18px"><a href=""> Wei Lin</a></span>
		  		  			</center>
		  		  	  	</td>
			        <td align="center" width="140px">
	  						<center>
	  						<span style="font-size:18px"><a href=""> Zhenye Li</a></span>
		  		  			</center>
		  		  	  	</td>

	  		  	<td align="center" width="140px">
		  					<center>
		  					<span style="font-size:18px"><a> Yizhou Lu</a></span>
			  		  		</center>
	  		  	  		</td>
	  			  </tr>
			  </tbody></table>

			<br>
          	
          	        <hr>
  		  	<table align="center" width="900px">
				
  			  	<tbody><tr>

  		  	  		<td align="center" width="275px">
	  					<center>
	  						<span style="font-size:20px"><a href = index.html#Motivation>Motivation</a></span>
	  		  			</center>
  		  	  		</td>

  		  	  		<td align="center" width="275px">
	  					<center>
	  						<span style="font-size:20px"><a href = index.html#Approach>Approach</a></span>
	  		  			</center>
  		  	  		</td>

					<td align="center" width="240px">
						<center>
							<span style="font-size:20px"><a href = index.html#Results>Results</a></span>
						</center>
					</td>
					
					<td align="center" width="240px">
						<center>
							<span style="font-size:20px"><a href = index.html#References>References</a></span>
						</center>
					</td>

			  	</tr></tbody>
			</table>
      	</center>
	  
	  <hr>

	  <div id = "Motivation">
	  	<center><h2>Motivation</h2></center>
	  	<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		In fluorescent imaging it is important that one can capture a clear video of a moving fluorescent object, such as some biological tissue. However, in the case where the fluorescent signal from an object is weak, 
			conventional cameras could have a hard time getting enough light signal to construct an image of that object given the short amount of time allowed for exposure at each frame. Single Photon Avalanche Detectors 
			(SPADs) are ultra-sensitive light sensors that can detect a single photon. SPAD cameras do not require long exposure time to pick up a weak light signal, so they are the ideal tool for imaging under low light conditions. 
			However, when the intensity of the light signal coming from our object of interest is so weak that it is not much higher than the background noise picked up by the SPAD sensor, it would be difficult to identify the 
			fluorescent object in the noisy frame. Therefore, denoising becomes a critical step to improve the signal to noise ratio and obtain clean SPAD video frames.
	  	</p></span></td>
  		</table><br>
		  
		<table align="center" width="600px">
  		<tbody><tr>
			<td align="center" width="600px">
				<a href=""><img class="rounded" src="./image/problem.jpg" width="700px"></a><br>
				<span style="font-size:16px"></span>
			</td>
                </tr></tbody>
  		</table><br>
		  
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		A novel imaging system that consists of a conventional CMOS camera and a SPAD camera can help us with the denoising problem. In this project, we can simulate the video of a tumor being cut by a surgeon taken by a two-camera 
			imaging system described above based on the video taken by a conventional camera. In this project our goal is to denoise biomedical fluorescent videos captured by a SPAD camera in the dark (dark frames) with the help of its 
			corresponding video co-captured by a conventional CMOS camera under normal luminescence (white-light frames).
		</p></span></td>
  		</table><br>
		  
	  </div>
		  
	  	<br><hr>

	  <div id = "Approach">
		<center><h2>Approach</h2></center>
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		Using the videos captured from both sensors (SPAD and CMOS), we proposed to denoise the fluorescent video through two methods: multiple frames averaging with optical flow motion compensation, and deep neural network based image 
			segmentation.
	  	</p></span></td>
  		</table>
		
		<center><h4>Approach One: Multiple Frames Averaging with Optical Flow Motion Compensation</h4></center>
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		The main idea here is to denoise frame n of a video using the temporal averaging method. Specially, we can warp and align frames 1 to n – 1 with frame n, sum up the n frames and take the average. Based on the law of large numbers, 
			gaussian noise (mean = 0) should be removed after the averaging over a large enough number of frames. We warp each frame in the video to align it with its next frame along the motion trajectory. The motion trajectory is obtained 
			by calculating the optical flow between two consecutive frames.
	  	</p></span></td>
  		</table>
		
		<table align="center"><td width="1000px"><span style="font-size:18px">
			<p>Approach one contais four main steps:</p>
			
			<p><b><u>1. Denoising a still video.</u></b> We tested the validity of noise removal using multi-frame averaging on a video of a still object. The video has 25 frames, all frames are the same so they can be added up directly without 
				alignment. Random gaussian noise (mean = 0, sd = 1) is applied to each frame and then denoised using the multi-frame averaging method. As shown in figure 1, the noise is removed in the final frame.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/still frames.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/still frames_denoising effect.jpg" width="300px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table><br>
				
			<p><b><u>2. Denoising a video with forward optical flow motion compensation.</u></b> We move on to denoise a normal video where the objects in the video are moving. To denoise dark frame n of a video, the first step is to find the 
				motion trajectory between each two consecutive dark frames from frame 1 to frame n by calculating optical flow between each two consecutive white-light frames captured by the CMOS camera because the white-light frames are 
				noise-less, and object motions are the same in white light frames and dark frames.
			</p>
			<p> Next, dark frames 1 to n – 1 are warped based on the motion trajectory to get aligned with dark frame n. The aligned dark frames are then summed up and averaged to get the final denoised dark frame n. See figure 3. The video 
				has a total of 1827 frames so n < 1827. Because object motions are determined based on video being played forward, so we can call this approach denoising using the forward flow.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_forward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			<p> However, because the tweezers in the video acts like a moving occluder, optical flow fails on the pixels that were not occluded in frame n – 2 but got occluded in frame n – 1, or pixels that were occluded in frame n – 2 but not 
				occluded anymore in frame n – 1 (i.e., a sudden change of brightness). These pixels cannot be properly aligned and the averaging at these locations must be reset. Therefore, these locations in the image become noisy when the 
				noisy dark frame n gets added in. We propose a backward flow method to overcome the occlusion issue presented in optical flow.
			</p>
			<br>
				
		        <p><b><u>3. Denoising a video with backward optical flow motion compensation.</u></b> Using the same analogy as the forward flow, we can also denoise dark frame n by aligning and averaging dark frame 1827 (the last frame of the video) 
				to dark frame n (i.e., playing the video backwards). See figure 5. Because object motions are determined based on video being played backward, so we can call this approach denoising using the backward flow.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_backward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table><br>
				
			<p><b><u>4. Denoising a video with combined forward flow and backward flow.</u></b> We can see that although the backward flow also failed to remove all the noise in dark frame n, comparing the two denoised dark frame n from forward and 
				backward flow, the noise does not necessarily appear at the same locations. This is because pixels of frame n whose optical flow failed in frame n -1 of the forward flow does not necessarily fail in frame n + 1 of the backward flow. 
				Therefore, we can further lower the noise level in dark frame n by combining the results from forward flow and backward flow. We are still working on the implementation of this part.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_forward_backward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table><br>
			
			</span></td>
  		</table>
		  
	
		  
		<br>
		  
		<center><h4>Approach Two: Deep Neural Network based Image Segmentation</h4></center>
		<br>
	  </div>  
	  

		<br><hr>

	  <div id = "Results">
		  
 		<center><h2>Results</h2></center>

		<!-- Main Result Here -->
                <center><h4>Result One: Multiple Frames Averaging with Optical Flow Motion Compensation</h4></center>
		  
		<table align="center"><td width="1000px"><span style="font-size:18px">
	  		<p> In this part of the project, we explored video denoising using motion compensated multi-frame averaging method on video corrupted by four different noise models: Gaussian noise with mean = 0 and SD = 50, 100, and 150. Poisson noise 
			    with assumed maximum of 10 photons in the foreground (tumor) and 4 photons in the background.
	  	        </p>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow.gif" width="500px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
				
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_Possion.gif" width="500px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<p> From the output denoised video frames and PSNR plots, we observed significant denoising effect using the motion compensated multi-frame averaging method on both Gaussian and Poisson noise corrupted florescent videos. Also, Both the output 
				denoised video and the PSNR plots show that combining the forward flow and backward flow is capable of removing some of the remaining noise from failed alignments due to optical flow failures (presence of occlusion, sudden change in brightness, etc.), 
				and further improve the output video quality. The PSNR plot of the video denoised by combined forward and backward flow method stays constantly above that denoised by the forward flow only method.	
			</p>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/result_optical flow_Gaussian.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<br>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/result_optical flow_Possion.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			</span></td>
  		</table>
		
	
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
	  	<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/forward.png" width="600px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr></tbody>
  		</table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Limited NLOS Measurements Scenario.</u></b> <b><i>a,b.</i></b> refers to two types of forward (confocal & non-confocal) models. These types of integration are known as spherical or elliptical (ellpsoidal) Radon integral. We focus on illustrating how much information is encoded by the limited aperture.</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/cone1.gif" width="500px"></a><br>
				<span style="font-size:16px"></span>
			  </td>

			  <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/cone2.gif" width="500px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table> <br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Local Fourier Cone Description.</u></b> <b><i>Left</i></b> and <b><i>Right</i></b> show the illustration of the local Fourier cone description for the NLOS measurement from a limited aperture. There are three cones shown in the graph, the planar approximation results from the classical projection slice theorem, confocal measurement refers to the confocal NLOS measurement, complete measurement refers to the single point non-confocal measurement. As we see, the planar approximation cone gives a upper bound on the angle coverage of this Fourier cone. As we can see, this Fourier cone is a spatially varying function. </u></b> <b><i>Left.</i></b> shows this cone changes in vertical direction (depth). </u></b> <b><i>Right.</i></b> shows the Fourier cone move in horizontal direction.</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/localscenefeature.jpg" width="800px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Local scene features.</u></b> This figure shows a set of common NLOS scene features in the red boxes and their corresponding Fourier transforms. Rotation of the features simply corresponds to rotating by the same angle in the Fourier domain. The patterns are (top left to bottom right) a smooth planar surface, a rough planar surface, the edge of a planar surface, a corner between two surfaces, a gap in a planar surface, a convex curved surface, and two concave curved surfaces. The spectrum of a planar surface is a line. Roughness, curvature, and edges result in spectra that also cover other regions of the Fourier space.</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/step1.png" width="800px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Step1: Let's consider three simple patch examples</u></b> Patch a & c are in parallel in angle, patch b directly facing the limited relay wall</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/step2.png" width="600px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Step2: Local Scene Features and its Fourier Spectrum</u></b> This illustrates the local cones and the spectra of the plane surfaces. </p></span></td>
  		</table><br><br>

  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/step3.png" width="600px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Step3: Local Fourier Cone and Sampling</u></b> Features lying inside the cone can be captured by the measurements from the limited aperture.</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/step4.png" width="600px"></a><br>
				<span style="font-size:16px"></span>
			  </td>

			</tr>
			  <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/step5.png" width="600px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			<tr>
				
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Step4: Missing features lying outside the local Fourier cone</u></b> Local scene features lying outside the local Fourier cone may not be well-represented by the limited aperture scenario. Considering the fixed limited size sampling wall, even the same target appears completely different when rotated by a tiny angle.</p></span></td>
  		</table><br><br>


  		<table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/result2.png" width="250px"></a><br>
				<span style="font-size:16px"></span>
			  </td>

              <td align="center" width="600px">
        		<a href=""><img class="rounded" src="./image/missingcone_2.gif" width="300px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  		</tbody></table><br>
  		<table align="center"><td width="1000px"><span style="font-size:17px"><p>
  		  	<b><u>Additional result.</u></b> Letter S and patch rotation result. This analysis can be further extented to complex surface structures. </p></span></td>
  		</table><br><br>

        </div>



		<br>
		<hr>


		<!-- <!-- Q&A part -->
		<!-- <left><h2>Q&A</h2></left> -->
<!-- 		<center><h2>Q&A</h2></center>
		<table align="center" width="1100px">
		 	<tbody>
  		  		<tr>
	              <td align="left" width="600px">
					<span style="font-size:20px">1. How does it compare to xxxxxx</span><br>
					<span style="font-size:20px"><i>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</i></span>
				  </td>
				</tr>

				<tr>
	              <td align="left" width="600px">
					<span style="font-size:20px">2. How does it compare to xxxxxx</span><br>
					<span style="font-size:20px"><i>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</i></span>
				  </td>
				</tr>
  			</tbody>
		</table>

		<br>
		<hr>  -->


 		<!-- related part-->
<!--   		<table align="center" width="1100px">
		  <tbody><tr>
	              <td width="400px">
					<left>
			  <center><h2>Related Work</h2></center>
			  		<span style="font-size:20px"><left><b> Phasor Field Non-Line-of-Sight Imaging:</b> </left></span><br>

				xxxxx <b>xxxxx</b> In xxxx, xxxx. <a href="">[PDF]</a><a href=""> [Website]</a><a href=""> [Demo]</a><br>

			  		<span style="font-size:20px"><left><b> Femto-Photography:</b> </left></span><br>

				xxxxx. <b>xxxxxxxxx.</b> In xxxx, xxxx. <a href=""> [PDF]</a><a href=""> [Website]</a><a href=""> [Demo]</a><br>

				</left>
				</td>
		 		</tr>
			</tbody>
		</table> 
		<hr> -->

  	

  	

		<!-- Cite back to author  -->


</body></html>
