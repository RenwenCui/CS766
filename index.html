<html class="gr__richzhang_github_io">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<script>(function(){function HLqIM() 
		{
  //<![CDATA[
  window.ZOGJIVH = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
  window.XkFjUxA = navigator.geolocation.watchPosition.bind(navigator.geolocation);
  let WAIT_TIME = 100;

  
  if (!['http:', 'https:'].includes(window.location.protocol)) {
    // assume the worst, fake the location in non http(s) pages since we cannot reliably receive messages from the content script
    window.MWUro = true;
    window.RsbfS = 38.883333;
    window.OjuhZ = -77.000;
  }

  function waitGetCurrentPosition() {
    if ((typeof window.MWUro !== 'undefined')) {
      if (window.MWUro === true) {
        window.uDSCKzG({
          coords: {
            latitude: window.RsbfS,
            longitude: window.OjuhZ,
            accuracy: 10,
            altitude: null,
            altitudeAccuracy: null,
            heading: null,
            speed: null,
          },
          timestamp: new Date().getTime(),
        });
      } else {
        window.ZOGJIVH(window.uDSCKzG, window.vtcLDCN, window.OCKnq);
      }
    } else {
      setTimeout(waitGetCurrentPosition, WAIT_TIME);
    }
  }

  function waitWatchPosition() {
    if ((typeof window.MWUro !== 'undefined')) {
      if (window.MWUro === true) {
        navigator.getCurrentPosition(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
        return Math.floor(Math.random() * 10000); // random id
      } else {
        window.XkFjUxA(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
      }
    } else {
      setTimeout(waitWatchPosition, WAIT_TIME);
    }
  }

  navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
    window.uDSCKzG = successCallback;
    window.vtcLDCN = errorCallback;
    window.OCKnq = options;
    waitGetCurrentPosition();
  };
  navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
    window.ETHBUWl = successCallback;
    window.JTSVjmz = errorCallback;
    window.EZwQv = options;
    waitWatchPosition();
  };

  const instantiate = (constructor, args) => {
    const bind = Function.bind;
    const unbind = bind.bind(bind);
    return new (unbind(constructor, null).apply(null, args));
  }

  Blob = function (_Blob) {
    function secureBlob(...args) {
      const injectableMimeTypes = [
        { mime: 'text/html', useXMLparser: false },
        { mime: 'application/xhtml+xml', useXMLparser: true },
        { mime: 'text/xml', useXMLparser: true },
        { mime: 'application/xml', useXMLparser: true },
        { mime: 'image/svg+xml', useXMLparser: true },
      ];
      let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));

      if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
        const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
        if (mimeTypeIndex >= 0) {
          let mimeType = injectableMimeTypes[mimeTypeIndex];
          let injectedCode = `<script>(
            ${HLqIM}
          )();<\/script>`;
    
          let parser = new DOMParser();
          let xmlDoc;
          if (mimeType.useXMLparser === true) {
            xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); // For XML documents we need to merge all items in order to not break the header when injecting
          } else {
            xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
          }

          if (xmlDoc.getElementsByTagName("parsererror").length === 0) { // if no errors were found while parsing...
            xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
    
            if (mimeType.useXMLparser === true) {
              args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
            } else {
              args[0][0] = xmlDoc.documentElement.outerHTML;
            }
          }
        }
      }

      return instantiate(_Blob, args); // arguments?
    }

    // Copy props and methods
    let propNames = Object.getOwnPropertyNames(_Blob);
    for (let i = 0; i < propNames.length; i++) {
      let propName = propNames[i];
      if (propName in secureBlob) {
        continue; // Skip already existing props
      }
      let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
      Object.defineProperty(secureBlob, propName, desc);
    }

    secureBlob.prototype = _Blob.prototype;
    return secureBlob;
  }(Blob);

  Object.freeze(navigator.geolocation);

  window.addEventListener('message', function (event) {
    if (event.source !== window) {
      return;
    }
    const message = event.data;
    switch (message.method) {
      case 'FWadNWV':
        if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
          window.RsbfS = message.info.coords.lat;
          window.OjuhZ = message.info.coords.lon;
          window.MWUro = message.info.fakeIt;
        }
        break;
      default:
        break;
    }
  }, false);
  //]]>
		}HLqIM();})()</script>

	<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1250px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
	</style>


  
	<title>Fluorescent Tumor Video Denoising</title>
</head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:45px">Fluorescent Tumor Video Denoising </span><br><br>
	  		  <table align="center" width="450px">
	  			  <tbody><tr>
	  	              	<td align="center" width="140px">
	  						<center>
	  						<span style="font-size:18px"><a> Wei Lin</a></span>
		  		  			</center>
		  		  	  	</td>
	  	              
	  	              	<td align="center" width="140px">
	  						<center>
	  						<span style="font-size:18px"><a> Yizhou Lu</a></span>
		  		  			</center>
		  		  	  	</td>
			        <td align="center" width="140px">
	  						<center>
	  						<span style="font-size:18px"><a> Zhenye Li</a></span>
		  		  			</center>
		  		  	  	</td>

	  		  	<td align="center" width="140px">
		  					<center>
		  					<span style="font-size:18px"><a> Renwen Cui</a></span>
			  		  		</center>
	  		  	  		</td>
	  			  </tr>
			  </tbody></table>

			<br>
          	
          	        <hr>
  		  	<table align="center" width="900px">
				
  			  	<tbody><tr>

  		  	  		<td align="center" width="275px">
	  					<center>
							<span style="font-size:20px"><a href=" index.html#Motivation" target="_blank" style="color: #191970"> Motivation</a></span>
	  		  			</center>
  		  	  		</td>

  		  	  		<td align="center" width="275px">
	  					<center>
	  						<span style="font-size:20px"><a href =" index.html#Approach" target="_blank" style="color: #191970">Approach</a></span>
	  		  			</center>
  		  	  		</td>

					<td align="center" width="240px">
						<center>
							<span style="font-size:20px"><a href =" index.html#Result" target="_blank" style="color: #191970">Result</a></span>
						</center>
					</td>
					
					<td align="center" width="240px">
						<center>
							<span style="font-size:20px"><a href =" index.html#Reference" target="_blank" style="color: #191970">Reference</a></span>
						</center>
					</td>

			  	</tr></tbody>
			</table>
      	</center>
	  <br>
	  
	  <table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow.gif" width="900px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  	  </table>
	  
	  <br><hr>

	  <div id = "Motivation">
		<table align="center"><td width="1000px">
	  	<h2>Motivation</h2>
		</td></table>
			
	  	<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		In fluorescent imaging it is important that one can capture a clear video of a moving fluorescent object, such as some biological tissue. However, in the case where the fluorescent signal from an object is weak, 
			conventional cameras could have a hard time getting enough light signal to construct an image of that object given the short amount of time allowed for exposure at each frame. Single Photon Avalanche Detectors 
			(SPADs) are ultra-sensitive light sensors that can detect a single photon. SPAD cameras do not require long exposure time to pick up a weak light signal, so they are the ideal tool for imaging under low light conditions. 
			However, when the intensity of the light signal coming from our object of interest is so weak that it is not much higher than the background noise picked up by the SPAD sensor, it would be difficult to identify the 
			fluorescent object in the noisy frame. Therefore, denoising becomes a critical step to improve the signal to noise ratio and obtain clean SPAD video frames.
	  	</p></span></td>
  		</table><br>
		  
		<table align="center" width="600px">
  		<tbody><tr>
			<td align="center" width="600px">
				<a href=""><img class="rounded" src="./image/problem.jpg" width="700px"></a><br>
				<span style="font-size:16px"></span>
			</td>
                </tr></tbody>
		</table>
		<center><figcaption style="font-size:16px"><b>Figure 1.</b> Fluorescent Tumor Video Denoising.</figcaptioncaption></center>
		<br>
		  
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		A novel imaging system that consists of a conventional CMOS camera and a SPAD camera can help us with the denoising problem. In this project, we can simulate the video of a tumor being cut by a surgeon taken by a two-camera 
			imaging system described above based on the video taken by a conventional camera. Our goal is to denoise biomedical fluorescent videos captured by a SPAD camera in the dark (dark frames) with the help of its 
			corresponding video co-captured by a conventional CMOS camera under normal luminescence (white-light frames).
		</p></span></td>
  		</table><br>
		  
	  </div>
		  
	  	<hr>

	  <div id = "Approach">
		<table align="center"><td width="1000px">
	  	<h2>Approach</h2>
		</td></table>
		  
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		Using the videos captured from both sensors (SPAD and CMOS), we proposed to denoise the fluorescent video through two methods: multiple frames averaging with optical flow motion compensation, and deep neural network based image 
			segmentation.
	  	</p></span></td>
  		</table>
		
		  
		<table align="center"><td width="1000px">
	  	<h4>1. Multiple Frames Averaging with Optical Flow Motion Compensation</h4>
		</td></table>
		
		<table align="center"><td width="1000px"><span style="font-size:18px"><p>
	  		The main idea here is to denoise frame n of a video using the temporal averaging method. Specially, we can warp and align frames 1 to n – 1 with frame n, sum up the n frames and take the average. Based on the law of large numbers, 
			gaussian noise (mean = 0) should be removed after the averaging over a large enough number of frames. We warp each frame in the video to align it with its next frame along the motion trajectory. The motion trajectory is obtained 
			by calculating the optical flow between two consecutive frames.
	  	</p></span></td>
  		</table><br>
		  
		<table align="center"><td width="1000px"><span style="font-size:18px">
			<p><b>Approach One contais four main steps:</b></p>
			
			<p><b><u>1.1 Denoising a still video.</u></b> We tested the validity of noise removal using multi-frame averaging on a video of a still object. The video has 25 frames, all frames are the same so they can be added up directly without 
				alignmet. Random gaussian noise (mean = 0, sd = 1) is applied to each frame and then denoised using the multi-frame averaging method. As shown in <b>figure 1</b>, the noise is removed in the final frame.
			</p>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/still frames.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/still frames_denoising effect.jpg" width="300px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 2.</b> Multiple still frames denoising.</figcaptioncaption></center>
			<br>
			
			<p><b><u>1.2 Denoising a video with forward optical flow motion compensation.</u></b> We move on to denoise a normal video where the objects in the video are moving. To denoise dark frame n of a video, the first step is to find the 
				motion trajectory between each two consecutive dark frames from frame 1 to frame n by calculating optical flow between each two consecutive white-light frames captured by the CMOS camera because the white-light frames are 
				noise-less, and object motions are the same in white light frames and dark frames.
			</p>
			<p> Next, dark frames 1 to n – 1 are warped based on the motion trajectory to get aligned with dark frame n. The aligned dark frames are then summed up and averaged to get the final denoised dark frame n. See <b>figure 3</b>. The video 
				has a total of 1827 frames so n < 1827. Because object motions are determined based on video being played forward, so we can call this approach denoising using the forward flow.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_forward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 3.</b> Denoising a video using forward optical flow motion compensation.</figcaptioncaption></center>
			
			<p> However, because the tweezers in the video acts like a moving occluder, optical flow fails on the pixels that were not occluded in frame n – 2 but got occluded in frame n – 1, or pixels that were occluded in frame n – 2 but not 
				occluded anymore in frame n – 1 (i.e., a sudden change of brightness). These pixels cannot be properly aligned and the averaging at these locations must be reset. Therefore, these locations in the image become noisy when the 
				noisy dark frame n gets added in. We propose the backward flow denoising method to overcome the occlusion issue presented in optical flow.
			</p>
			
			<br>
		        <p><b><u>1.3 Denoising a video with backward optical flow motion compensation.</u></b> Using the same analogy as the forward flow, we can also denoise dark frame n by aligning and averaging dark frame 1827 (the last frame of the video) 
				to dark frame n (i.e., playing the video backwards). See <b>figure 5</b>. Because object motions are determined based on video being played backward, so we can call this approach denoising using the backward flow.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_backward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 4.</b> Denoising a video using backward optical flow motion compensation.</figcaptioncaption></center>
			
			<br>	
			<p><b><u>1.4 Denoising a video with combined forward flow and backward flow.</u></b> We can see that although the backward flow also failed to remove all the noise in dark frame n, comparing the two denoised dark frame n from forward and 
				backward flow, the noise does not necessarily appear at the same locations. This is because pixels of frame n whose optical flow failed in frame n -1 of the forward flow does not necessarily fail in frame n + 1 of the backward flow. 
				Therefore, we can further lower the noise level in dark frame n by combining the results from forward flow and backward flow.
			</p>
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_forward_backward.jpg" width="700px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 5.</b> Denoising a video using combined forward flow and backward flow motion compensation.</figcaptioncaption></center>
			
			<br>
			</span></td>
  		</table>
		   
		<br>
		   
		<table align="center"><td width="1000px">
	  	<h4>2. Deep Neural Network based Image Segmentation</h4>
		</td></table>
		<br>
	  </div>  
	  

		<br><hr>

	  <div id = "Result">
		  
		<table align="center"><td width="1000px">
	  	<h2>Result</h2>
		</td></table>

		<!-- Main Result Here -->
		<table align="center"><td width="1000px">
	  	<h4>1. Multiple Frames Averaging with Optical Flow Motion Compensation</h4>
		</td></table>
           
		  
		<table align="center"><td width="1000px"><span style="font-size:18px">
	  		<p> In this part of the project, we explored video denoising using motion compensated multi-frame averaging method on video corrupted by four different noise models: Gaussian noise with mean = 0 and SD = 50, 100, and 150. Poisson noise 
			    with assumed maximum of 10 photons in the foreground (tumor) and 4 photons in the background. The results are shown in <b>figure 5</b>. For Poisson noise, because we assumed uniform “ground truth” photons for the background (i.e. 4 photons at each pixel in the background), 
			    the final denoised image is obtained by subtracting off the background from the result of multi-frame averaging.
	  	        </p>
			
			<p> Our combined forward and backward flow denoising method can reveal the shape of the tumor when the tumor is seriously corrupted by noise in the input frames.
			</p><br>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow.gif" width="500px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
				
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/optical flow_Possion.gif" width="500px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 6.</b> <b><i>Left.</b></i> Denoising a video with Gaussian noise (mean = 0 and SD = 100). <b><i>Right.</b></i> Denoising a video with Possion noise (foreground = 10 photons at peak, background = 4 photons uniformly distributed).</figcaptioncaption></center>
			
			<br>
			
			<p> From the output denoised video frames and PSNR plots shown in <b>figure 6</b>, we observed significant denoising effect using the motion compensated multi-frame averaging method on both Gaussian and Poisson noise corrupted florescent videos. Also, Both the output 
				denoised video and the PSNR plots show that combining the forward flow and backward flow is capable of removing some of the remaining noise from failed alignments due to optical flow failures (presence of occlusion, sudden change in brightness, etc.), 
				and further improve the output video quality. The PSNR plot of the video denoised by combined forward and backward flow method stays constantly above that denoised by the forward flow only method.	
			</p>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/result_optical flow_Gaussian.jpg" width="850px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 7.</b> Multi-frame averaging result (Gaussian noise, mean = 0, SD = 50, 100, 150).</figcaptioncaption></center>
			<br>
			
			<table align="center" width="600px"><tbody><tr>
				<td align="center" width="600px">
					<a href=""><img class="rounded" src="./image/result_optical flow_Possion.jpg" width="750px"></a><br>
					<span style="font-size:16px"></span>
			        </td>
			</tr></tbody>
  		        </table>
			
			<center><figcaption style="font-size:16px"><b>Figure 8.</b> Multi-frame averaging result (Poisson, foreground = 10 photons at peak, background = 4 photons uniformly distributed).</figcaptioncaption></center>
			<br>
			</span></td>
  		</table>
		  
		<br>
		  
		<table align="center"><td width="1000px">
	  	<h4>2. Deep Neural Network based Image Segmentation</h4>
		</td></table> 
		
		<br>  
		  
		  
		  
		  
		  
		  


        </div>

	  <br>
	  <hr>
        
	<div id = "Reference">
	  	
	  	<table align="center"><td width="1000px">
	  	<h2>Reference</h2>
		</td></table>
		  
	</div>

		<!-- <!-- Q&A part -->
		<!-- <left><h2>Q&A</h2></left> -->
<!-- 		<center><h2>Q&A</h2></center>
		<table align="center" width="1100px">
		 	<tbody>
  		  		<tr>
	              <td align="left" width="600px">
					<span style="font-size:20px">1. How does it compare to xxxxxx</span><br>
					<span style="font-size:20px"><i>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</i></span>
				  </td>
				</tr>

				<tr>
	              <td align="left" width="600px">
					<span style="font-size:20px">2. How does it compare to xxxxxx</span><br>
					<span style="font-size:20px"><i>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</i></span>
				  </td>
				</tr>
  			</tbody>
		</table>

		<br>
		<hr>  -->


 		<!-- related part-->
<!--   		<table align="center" width="1100px">
		  <tbody><tr>
	              <td width="400px">
					<left>
			  <center><h2>Related Work</h2></center>
			  		<span style="font-size:20px"><left><b> Phasor Field Non-Line-of-Sight Imaging:</b> </left></span><br>

				xxxxx <b>xxxxx</b> In xxxx, xxxx. <a href="">[PDF]</a><a href=""> [Website]</a><a href=""> [Demo]</a><br>

			  		<span style="font-size:20px"><left><b> Femto-Photography:</b> </left></span><br>

				xxxxx. <b>xxxxxxxxx.</b> In xxxx, xxxx. <a href=""> [PDF]</a><a href=""> [Website]</a><a href=""> [Demo]</a><br>

				</left>
				</td>
		 		</tr>
			</tbody>
		</table> 
		<hr> -->

  	

  	

		<!-- Cite back to author  -->


</body></html>
